{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AaronDebattista09/ICS5200/blob/main/MGEC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HfEP8BcVmcS"
      },
      "source": [
        "# Preparation\n",
        "\n",
        "These cells execute some preparatory steps. Ideally, you should interface to a Google Drive so that Marian NMT can save model checkpoints if and when required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9DYwSbMVv-a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glUP2hjfV2cG"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "!mkdir -p GIT/\n",
        "!mkdir -p GIT/ICS5200\n",
        "\n",
        "%cd GIT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to install git-lfs, otherwise we might miss out on very large files."
      ],
      "metadata": {
        "id": "33geo02_iahx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TvOOitOcUuR"
      },
      "outputs": [],
      "source": [
        "!git init\n",
        "!apt-get install git-lfs\n",
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxFvQnPaZWIv"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/GIT/ICS5200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x7XyPvrZLDb"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AaronDebattista09/ICS5200.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFeaqX0QFVc8"
      },
      "source": [
        "# Build\n",
        "\n",
        "These cells execute the steps required to build Marian, including all pre-requisites and the reinstallation of CMake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1IKVAYx7aMJ"
      },
      "outputs": [],
      "source": [
        "# Install Debugger\n",
        "\n",
        "!pip install remote-pdb\n",
        "!pip install boto3\n",
        "!pip install urllib3\n",
        "\n",
        "!pip install fairseq\n",
        "!pip install omegaconf\n",
        "!pip install hydra\n",
        "!pip install hydra-core --upgrade\n",
        "!pip install bitarray\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuNnX5-XBHQk"
      },
      "outputs": [],
      "source": [
        "%cd /content/GIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1vP8c5E4gcM"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/grammatical/pretraining-bea2019.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm_wCEI05qPz"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install git cmake build-essential libboost-system-dev libprotobuf10 protobuf-compiler libprotobuf-dev openssl libssl-dev libgoogle-perftools-dev "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-7l6Ww97t2y"
      },
      "outputs": [],
      "source": [
        "!sudo apt remove --purge --auto-remove cmake\n",
        "!sudo apt update && \\\n",
        "!sudo apt install -y software-properties-common lsb-release && \\\n",
        "!sudo apt clean all\n",
        "\n",
        "!get -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/kitware.gpg >/dev/null\n",
        "!sudo apt-add-repository \"deb https://apt.kitware.com/ubuntu/ bionic main\"\n",
        "\n",
        "!sudo apt update\n",
        "!sudo apt install kitware-archive-keyring\n",
        "!sudo rm /etc/apt/trusted.gpg.d/kitware.gpg\n",
        "\n",
        "!sudo apt update\n",
        "!sudo apt install cmake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxAPa0B7W9lr"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get remove libtcmalloc-minimal4\n",
        "!sudo apt-get install libtcmalloc-minimal4\n",
        "!cp /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOyxy0zCKi2e"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/GIT/ICS5200/Overwrite/Makefile\" \"/content/GIT/pretraining-bea2019/systems/tools/Makefile\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-E1hrQ-74xTA"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/GIT/pretraining-bea2019/systems/tools/errant\n",
        "!rm -rf /content/GIT/pretraining-bea2019/systems/tools/jfleg\n",
        "!rm -rf /content/GIT/pretraining-bea2019/systems/tools/m2scorer\n",
        "\n",
        "%cd /content/GIT/pretraining-bea2019/systems/tools\n",
        "!make all -f /content/GIT/pretraining-bea2019/systems/tools/Makefile\n",
        "\n",
        "!rm -rf /content/GIT/pretraining-bea2019/systems/tools/marian-dev\n",
        "!make marian-dev -f /content/GIT/pretraining-bea2019/systems/tools/Makefile\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wasp5SuC2Axo"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "These cells load up data from Korpus Malti V3. They do not need to be executed since the processed files are already in the Git repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFqvvbLk-C-U"
      },
      "source": [
        "### MLRS Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_1tD5rS_Fq1"
      },
      "outputs": [],
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install spacy-langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lfud-3ceghmM"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOalWjQy-X8D"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "\n",
        "DL = \"Downloads\"\n",
        "DL_ZIP = DL + \"/zip\"\n",
        "DL_FILES = DL + \"/files\"\n",
        "\n",
        "!mkdir -p $DL\n",
        "!mkdir -p $DL_ZIP\n",
        "!mkdir -p $DL_FILES\n",
        "!rm -rf $DL_ZIP/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9gL3MuP-enj"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "\n",
        "%cd /content/$DL_ZIP\n",
        "\n",
        "# &export=download\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=1EphgAV1WI5gQRqfFW502Nd2NfZmAuYDR&export=download&confirm=t' -O Academic.zip\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=1EWFF5eXq7QyFlfVTrja7wQ3Zzyf8CFt2&export=download&confirm=t' -O Culture.zip\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=1EpsLRf5bwtT4qL9CSoCZw4dw1pQJmxOg&export=download&confirm=t' -O European.zip \n",
        "!gdown 'https://drive.google.com/u/0/uc?id=1EMOxIu2nXRyIh7akCH6Tvu6ZxF_l-3j7&export=download&confirm=t' -O Law.zip\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=1ETH8get-DeSDDEoIxDrfb0kZC6yUnKX3&export=download&confirm=t' -O News.zip\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=1Epg8uHzZnWyRtyhLZEw2f8vyV2Cr8EFu&export=download&confirm=t' -O Opinion.zip\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=1EL_hI1I-LErAt3y4urX8p_zEBcorPuhw&export=download&confirm=t' -O Parliament.zip\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=1EAptpKju7NE7Gp0D9Gc0tO2Rtq5uEE8a&export=download&confirm=t' -O Religion.zip\n",
        "!gdown 'https://drive.google.com/u/0/uc?id=1EDjFM7XP0UI-eHtaFJwsazAStk5wYG8h&export=download&confirm=t' -O Sport.zip\n",
        "\n",
        "%cd /content/$DL_FILES\n",
        "\n",
        "!mkdir -p Academic\n",
        "!mkdir -p Culture\n",
        "!mkdir -p European\n",
        "!mkdir -p Law\n",
        "!mkdir -p News\n",
        "!mkdir -p Opinion\n",
        "!mkdir -p Parliament\n",
        "!mkdir -p Religion\n",
        "!mkdir -p Sport"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_z8k0C5-pi1"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/$DL_FILES/*\n",
        "!sudo apt-get install unzip\n",
        "\n",
        "%cd /content/$DL_ZIP\n",
        "\n",
        "!unzip -o Academic -d /content/$DL_FILES/Academic/ \n",
        "!unzip -o Culture -d /content/$DL_FILES/Culture/\n",
        "!unzip -o European -d /content/$DL_FILES/European/\n",
        "!unzip -o Law -d /content/$DL_FILES/Law/\n",
        "!unzip -o News -d /content/$DL_FILES/News/\n",
        "!unzip -o Opinion -d /content/$DL_FILES/Opinion/\n",
        "!unzip -o Parliament -d /content/$DL_FILES/Parliament/\n",
        "!unzip -o Religion -d /content/$DL_FILES/Religion/\n",
        "!unzip -o Sport -d /content/$DL_FILES/Sport/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqPiu7zX-tLb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "\n",
        "mlrs_file_list = []\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(\"/content/\" + DL_FILES):\n",
        "  for filename in [f for f in filenames if f.endswith(\".txt\")]:\n",
        "    mlrs_file_list.append(os.path.join(dirpath, filename))\n",
        "\n",
        "for file in mlrs_file_list:\n",
        "  os.rename(file, file[:-4].replace('.','_') + '.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR8ITqmV-wPE"
      },
      "outputs": [],
      "source": [
        "mlrs_file_list = []\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(\"/content/\" + DL_FILES):\n",
        "  for filename in [f for f in filenames if f.endswith(\".xml\")]:\n",
        "    mlrs_file_list.append(os.path.join(dirpath, filename))\n",
        "\n",
        "for file in mlrs_file_list:\n",
        "  print(\"Updating: \" + file)\n",
        "  with open(file, \"r+\") as f:\n",
        "      old = f.read() # read everything in the file\n",
        "      f.seek(0) # rewind\n",
        "      f.write(\"<document>\" + old.replace('&','&amp;') + \"</document>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-5BRMkp-10b"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "mlrs_tokens_array = []\n",
        "\n",
        "for xml_file in filter(lambda x: x, mlrs_file_list): # I can edit the filter if needed...\n",
        "\n",
        "  print(\"Processing: \" + xml_file)\n",
        "  tree = ET.parse(xml_file)\n",
        "  root = tree.getroot()\n",
        "\n",
        "  for p in root[0].findall('p'):\n",
        "    for s in p.findall('s'):\n",
        "      mlrs_sentence_array = []\n",
        "      for row in s.text.split('\\n'):\n",
        "        if row != '':\n",
        "          mlrs_sentence_array.append(row.split('\\t'))\n",
        "\n",
        "      mlrs_tokens_array.append(mlrs_sentence_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge28gkR--4gq"
      },
      "outputs": [],
      "source": [
        "mlrs_clean_text = []\n",
        "for token_list in map(lambda sentence: \n",
        "                      filter(lambda x: x, \n",
        "                             map(lambda ele: str(ele[0]).replace('\\x93','').replace('\\x94', '').replace('…','')\n",
        "                             , sentence))\n",
        "                      , mlrs_tokens_array):\n",
        "  mlrs_clean_text.append(\" \".join(list(token_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8AHYwpz-7E0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def check_to_remove(input_string):\n",
        "  \n",
        "  if bool(re.search(r'^[^a-zA-Z]*$', input_string)) \\\n",
        "    or bool(re.search(r'.*\"$', input_string)) \\\n",
        "    or bool(re.search(r'^(Sinfonija|4 Improvviżi op|Ouverture|It- Trota \" op).*$', input_string)):\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "def eliminate_text(input_string):\n",
        "  \n",
        "  output_string = input_string\n",
        "\n",
        "  if bool(re.search(r'^MR SPEAKER :', output_string)):\n",
        "    output_string = re.sub(r'^MR SPEAKER :', '', output_string)\n",
        "\n",
        "  if bool(re.search(r'( Onor\\. Membri : Aye )', output_string)):\n",
        "    output_string = re.sub(r'( Onor\\. Membri : Aye )', '', output_string)\n",
        "\n",
        "  if bool(re.search(r'( Onor\\. Membri : No )', output_string)):\n",
        "    output_string = re.sub(r'( Onor\\. Membri : No )', '', output_string)\n",
        "\n",
        "  if bool(re.search(r'( Onor\\. Membri : Iva )', output_string)):\n",
        "    output_string = re.sub(r'( Onor\\. Membri : Iva )', '', output_string)\n",
        "\n",
        "  if bool(re.search(r'^ONOR.+:', output_string)):\n",
        "    output_string = re.sub(r'^ONOR.+:', '', output_string)\n",
        "\n",
        "  if bool(re.search(r'^THE CHAIRMAN :', output_string)):\n",
        "    output_string = re.sub(r'^THE CHAIRMAN :', '', output_string)\n",
        "   \n",
        "  \n",
        "  output_string = re.sub('\\(.*\\)', '', output_string)\n",
        "\n",
        "  return output_string.strip()\n",
        "\n",
        "mlrs_regex_clean_text = list(map(eliminate_text, filter(check_to_remove, mlrs_clean_text)))\n",
        "\n",
        "for t in mlrs_regex_clean_text:\n",
        "  print(t)\n",
        "\n",
        "print(\"\\n\\nSanitised using Regex. Kept {0}/{1} rows.\".format(len(mlrs_regex_clean_text), len(mlrs_clean_text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dnqBOHB_KOj"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy_langdetect import LanguageDetector\n",
        "\n",
        "def get_lang_detector(nlp, name):\n",
        "    return LanguageDetector()\n",
        "\n",
        "Language.factory(\"language_detector\", func=get_lang_detector)\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('language_detector', last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bigC87PD_Mo_"
      },
      "outputs": [],
      "source": [
        "mlrs_lang_text = []\n",
        "\n",
        "count = 0\n",
        "for text in mlrs_regex_clean_text:\n",
        "  doc = nlp(text)\n",
        "  detect_language = doc._.language \n",
        "  detect_language[\"text\"] = text\n",
        "  mlrs_lang_text.append(detect_language)\n",
        "  count += 1\n",
        "\n",
        "  if count % 500 == 0 or count == len(mlrs_regex_clean_text):\n",
        "    print(\"Processed: {0}/{1}\".format(count, len(mlrs_regex_clean_text)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1wrwwny_OcU"
      },
      "outputs": [],
      "source": [
        "# Remove English text\n",
        "mlrs_filtered_text = list(map(lambda x: x['text'], \n",
        "  filter(lambda x: (x['language'] == 'en' and x['score'] <= 0.9) \n",
        "                    or x['language'] != 'en', mlrs_lang_text)))\n",
        "\n",
        "for text in mlrs_filtered_text:\n",
        "  print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04YzyrbclvxF"
      },
      "source": [
        "# Replication\n",
        "\n",
        "This section can be skipped. However, running these cells would peform a simple English translation using one of the Marian models that were used in the original study."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKy0LEuZJ-2Z"
      },
      "outputs": [],
      "source": [
        "%cd /content/GIT/pretraining-bea2019/systems/model.lowresource\n",
        "!./download.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0MhlV9Gh3il"
      },
      "outputs": [],
      "source": [
        "%cd /content/GIT/pretraining-bea2019/systems/\n",
        "\n",
        "!rm -rf /content/GIT/pretraining-bea2019/systems/runs/generation/low_resource/\n",
        "!mkdir -p /content/GIT/pretraining-bea2019/systems/runs/generation/low_resource/\n",
        "\n",
        "!echo \"Mary had a litle lamb .\" > runs/generation/low_resource/test_lr.in \n",
        "!./run.sh model.lowresource \\\n",
        "  runs/generation/low_resource/test_lr.in \\\n",
        "  runs/generation/low_resource/test_lr.out  -d 0 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5EGfD5Hdt6m"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/GIT/pretraining-bea2019/systems/runs/generation/low_resource\n",
        "\n",
        "!./tools/marian-dev/build/marian-scorer -m model.lowresource/rl1.npz \\\n",
        " -v model.lowresource/vocab.{spm,spm} \\\n",
        " --n-best --n-best-feature R2L1 \\\n",
        " --workspace 6000 --mini-batch-words 4000 \\\n",
        " -t runs/generation/low_resource/test_lr.in runs/generation/low_resource/test_lr.out.nbest0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurations for Implementation\n",
        "\n",
        "You can activate/deactivate certain adaptations from here. \n",
        "\n",
        "* **source_word_corruption** - Toggles the removal of words from source files.\n",
        "* **domain_error_adaptation** - Toggles the use of synthesised files.\n",
        "* **large_vocabs** - Toggle the use of larger vocabularies. REQUIRES domain_error_adaptation to be turned on.\n",
        "* **tied_embeddings** - Toggles the user of tied embeddings.\n",
        "* **pretrained** - Set value in (0,1,2).\n",
        "  * **0** - No pretraining.\n",
        "  * **1** - Pretrain from BERTu.\n",
        "  * **2** - Pretrain from mBERTu.\n",
        "* **load_eval_sets** - Adds the evaluation sets to the training data.\n",
        "\n",
        "#### Preconfigurations \n",
        "\n",
        "These preconfiguraitons are based on the experiments undertaken in the study.\n",
        "\n",
        "| Experiment          | Src.Corr | DE.Adapt | Lrg.Vocab | Tied.Emb | Pt.BERTu | Pt.mBERTu | Eval.Sets |\n",
        "| ------------------- | -------- | -------- | --------- | -------- | -------- | --------- | --------- |\n",
        "| Baselines           |    N     |    N     |     N     |    N     |    N     |     N     |     N     |\n",
        "| Base+Src.Corr       |    Y     |    N     |     N     |    N     |    N     |     N     |     N     |\n",
        "| Base+Tied.Emb       |    N     |    N     |     N     |    Y     |    N     |     N     |     N     |\n",
        "| Base+T.E+BERTu      |    N     |    N     |     N     |    Y     |    Y     |     N     |     N     |\n",
        "| Base+T.E+mBERTu     |    N     |    N     |     N     |    Y     |    N     |     Y     |     N     |\n",
        "| DE.Adapt            |    N     |    Y     |     N     |    N     |    N     |     N     |     N     |\n",
        "| DE.Adapt+Lrg.Vocab  |    N     |    Y     |     Y     |    N     |    N     |     N     |     N     |\n",
        "| DE.Adapt+T.E        |    N     |    Y     |     N     |    Y     |    N     |     N     |     N     |\n",
        "| DE.Adapt+T.E+BERTu  |    N     |    Y     |     N     |    Y     |    Y     |     N     |     N     |\n",
        "| DE.Adapt+T.E+mBERTu |    N     |    Y     |     N     |    Y     |    N     |     Y     |     N     |\n",
        "| Final               |    N     |    Y     |     N     |    Y     |    Y     |     N     |     Y     |"
      ],
      "metadata": {
        "id": "ueomvLFLHkT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_word_corruption = False #@param {type:\"boolean\"}\n",
        "domain_error_adaptation = True #@param {type:\"boolean\"}\n",
        "large_vocabs = False  #@param {type:\"boolean\"}\n",
        "tied_embeddings = True #@param {type:\"boolean\"}\n",
        "pretrained = 1 #@param {type:\"integer\"}\n",
        "load_eval_sets = True #@param {type:\"boolean\"} "
      ],
      "metadata": {
        "id": "yi-GEGW0HjbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSBjOWHbwkoZ"
      },
      "source": [
        "# Implementation (Maltese)\n",
        "\n",
        "These cells will setup the Maltese GEC system and all its prerequisites."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prerequisites"
      ],
      "metadata": {
        "id": "-bxGK9LdI3Do"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BIpn2JZUqMA"
      },
      "outputs": [],
      "source": [
        "%cd /content/GIT/pretraining-bea2019/training/\n",
        "\n",
        "!mkdir -p bpe\n",
        "\n",
        "%cd /content/GIT/pretraining-bea2019/systems/\n",
        "\n",
        "!mkdir -p model.lowresource.mt/log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVE0Tyoz70aX"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-splitter\n",
        "!pip install sentencepiece\n",
        "!pip install --upgrade git+https://github.com/cisnlp/simalign.git#egg=simalign"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1T6iliYdbZL"
      },
      "outputs": [],
      "source": [
        "%cd /content/GIT/ICS5200/Tools/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a helper method to facilitate tokenisation. In the event that code depracates, setting **use_rest** to True will call the tokeniser from an online API, but be warned that this approach is much slower."
      ],
      "metadata": {
        "id": "hM0XUTGmJgdO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrQFRari79-v"
      },
      "outputs": [],
      "source": [
        "from tokenisation import MTRegex, MTParTokenizer, MTWordTokenizer, MTSentenceTokenizer, MTSentencePieceTokenizer\n",
        "import urllib.parse\n",
        "import requests\n",
        "from IPython.display import clear_output\n",
        "\n",
        "tokeniser = MTWordTokenizer();\n",
        "tokeniser_api_link = \"https://mlrs.research.um.edu.mt/tools/mlrsapi/tokenise?text=\"\n",
        "\n",
        "def tokenise(path, use_rest=False):\n",
        "\n",
        "  tokens = []\n",
        "  with open(path, 'r', errors='replace') as f:\n",
        "\n",
        "    lines = f.readlines()\n",
        "    count = 0\n",
        "\n",
        "    for line in lines:\n",
        "\n",
        "      if (use_rest):\n",
        "        query = \"https://mlrs.research.um.edu.mt/tools/mlrsapi/tokenise?text=\" + urllib.parse.quote(line)\n",
        "        response = requests.get(query)\n",
        "        tokens.append(response.json()['result'])\n",
        "      else:\n",
        "        tokens.append(tokeniser.tokenize(line))\n",
        "      \n",
        "      count += 1\n",
        "      print(\"{0}/{1}\".format(count, len(lines)))\n",
        "\n",
        "    clear_output()\n",
        "    print(\"Processed: \" + path)\n",
        "  \n",
        "  return tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenisation"
      ],
      "metadata": {
        "id": "g9zP-Am3NLYA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaVDkTrQsK9Q"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "src_tokens = tokenise(\"/content/GIT/ICS5200/QariTalProvi_Batch_1/Curated/src.txt\")\n",
        "trg_tokens = tokenise(\"/content/GIT/ICS5200/QariTalProvi_Batch_1/Curated/trg.txt\")\n",
        "\n",
        "if domain_error_adaptation:\n",
        "  src_tokens = src_tokens + \\\n",
        "    tokenise(\"/content/GIT/ICS5200/Common Voice/Synthesized/src.txt\") + \\\n",
        "    tokenise(\"/content/GIT/ICS5200/MLRS/Synthesized/src.txt\")\n",
        "  \n",
        "  trg_tokens = trg_tokens + \\\n",
        "    tokenise(\"/content/GIT/ICS5200/Common Voice/Synthesized/trg.txt\") + \\\n",
        "    tokenise(\"/content/GIT/ICS5200/MLRS/Synthesized/trg.txt\")\n",
        "    \n",
        "if load_eval_sets:\n",
        "  eval_src_tokens = []\n",
        "  eval_trg_tokens = []\n",
        "  \n",
        "  lst_eval_src = glob.glob(\"/content/GIT/ICS5200/QariTalProvi_Batch_2/Training/Converted/Source/*.txt\")\n",
        "  lst_eval_trg = glob.glob(\"/content/GIT/ICS5200/QariTalProvi_Batch_2/Training/Converted/Target/*.txt\")\n",
        "  lst_eval_src.sort()\n",
        "  lst_eval_trg.sort()\n",
        "\n",
        "  for eval_src_file in lst_eval_src:\n",
        "    eval_src_tokens += tokenise(eval_src_file)\n",
        "  \n",
        "  for eval_trg_file in lst_eval_trg:\n",
        "    eval_trg_tokens += tokenise(eval_trg_file)\n",
        "\n",
        "  src_tokens = src_tokens + eval_src_tokens\n",
        "  trg_tokens = trg_tokens + eval_trg_tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(trg_tokens)"
      ],
      "metadata": {
        "id": "y75hIDCjy-TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-itkOJF8GFY"
      },
      "outputs": [],
      "source": [
        "with open('/content/GIT/pretraining-bea2019/training/src_tokenised.txt','w') as f:\n",
        "  c = 0\n",
        "  for line in src_tokens:\n",
        "    f.writelines(\" \".join(line) + \"\\n\")\n",
        "    c += 1\n",
        "  \n",
        "  print(\"Wrote {0} lines to file.\".format(c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny1glMfA8Ovm"
      },
      "outputs": [],
      "source": [
        "with open('/content/GIT/pretraining-bea2019/training/trg_tokenised.txt','w') as f:\n",
        "  c = 0\n",
        "  for line in trg_tokens:\n",
        "    f.writelines(\" \".join(line) + \"\\n\")\n",
        "    c += 1\n",
        "    \n",
        "  print(\"Wrote {0} lines to file.\".format(c))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test/Validate Splits"
      ],
      "metadata": {
        "id": "_vfuZWHjOV-H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX-zubuu8rwV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(src_tokens, trg_tokens, test_size=0.10)\n",
        "\n",
        "print(\"X Train: \" + str(len(x_train)))\n",
        "print(\"X Test: \" + str(len(x_test)))\n",
        "print(\"Y Train: \" + str(len(y_train)))\n",
        "print(\"Y Test: \" + str(len(y_test)))\n",
        "\n",
        "with open(\"/content/GIT/pretraining-bea2019/training/train.src\", \"w\", encoding=\"UTF-8\") as f:\n",
        "  f.writelines(list(map(lambda t: \" \".join(t) + \"\\n\",x_train)))\n",
        "\n",
        "with open(\"/content/GIT/pretraining-bea2019/training/train.trg\", \"w\", encoding=\"UTF-8\") as f:\n",
        "  f.writelines(list(map(lambda t: \" \".join(t) + \"\\n\",y_train)))\n",
        "\n",
        "with open(\"/content/GIT/pretraining-bea2019/training/valid.src\", \"w\", encoding=\"UTF-8\") as f:\n",
        "  f.writelines(list(map(lambda t: \" \".join(t) + \"\\n\",x_test)))\n",
        "\n",
        "with open(\"/content/GIT/pretraining-bea2019/training/valid.trg\", \"w\", encoding=\"UTF-8\") as f:\n",
        "  f.writelines(list(map(lambda t: \" \".join(t) + \"\\n\",y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcyseTD_9IMw"
      },
      "outputs": [],
      "source": [
        "# Check samples\n",
        "with open(\"/content/GIT/pretraining-bea2019/training/train.src\", \"r+\", encoding=\"UTF-8\") as train_src, \\\n",
        "  open(\"/content/GIT/pretraining-bea2019/training/train.trg\", \"r+\", encoding=\"UTF-8\") as train_trg, \\\n",
        "  open(\"/content/GIT/pretraining-bea2019/training/valid.src\", \"r+\", encoding=\"UTF-8\") as valid_src, \\\n",
        "  open(\"/content/GIT/pretraining-bea2019/training/valid.trg\", \"r+\", encoding=\"UTF-8\") as valid_trg:\n",
        "  \n",
        "  for conn in [train_src, train_trg, valid_src, valid_trg]:\n",
        "\n",
        "    lines = conn.readlines()\n",
        "\n",
        "    print(\"Sampling: \" + conn.name)\n",
        "    for line in lines[0:5]:  \n",
        "      print(line, end=\"\")\n",
        "\n",
        "    print(\"Lines: \" + str(len(lines)))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BPE Encoding"
      ],
      "metadata": {
        "id": "2OXNwTDAOYb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7SM2pnO-S9u"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/GIT/pretraining-bea2019/training/bpe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Kvv56IE975n"
      },
      "outputs": [],
      "source": [
        "!/content/GIT/subword-nmt/subword_nmt/learn_bpe.py  < /content/GIT/pretraining-bea2019/training/train.src > /content/GIT/pretraining-bea2019/training/bpe/train.bpe.src\n",
        "!/content/GIT/subword-nmt/subword_nmt/learn_bpe.py  < /content/GIT/pretraining-bea2019/training/train.trg > /content/GIT/pretraining-bea2019/training/bpe/train.bpe.trg\n",
        "!/content/GIT/subword-nmt/subword_nmt/learn_bpe.py < /content/GIT/pretraining-bea2019/training/valid.src > /content/GIT/pretraining-bea2019/training/bpe/valid.bpe.src\n",
        "!/content/GIT/subword-nmt/subword_nmt/learn_bpe.py < /content/GIT/pretraining-bea2019/training/valid.trg > /content/GIT/pretraining-bea2019/training/bpe/valid.bpe.trg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdSOiINc4Dkn"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/GIT/pretraining-bea2019/training/bpe/train.bpe.src\", \"r+\", encoding=\"UTF-8\") as train_src, \\\n",
        "  open(\"/content/GIT/pretraining-bea2019/training/bpe/train.bpe.trg\", \"r+\", encoding=\"UTF-8\") as train_trg, \\\n",
        "  open(\"/content/GIT/pretraining-bea2019/training/bpe/valid.bpe.src\", \"r+\", encoding=\"UTF-8\") as valid_src, \\\n",
        "  open(\"/content/GIT/pretraining-bea2019/training/bpe/valid.bpe.trg\", \"r+\", encoding=\"UTF-8\") as valid_trg:\n",
        "  \n",
        "  for conn in [train_src, train_trg, valid_src, valid_trg]:\n",
        "\n",
        "    lines = conn.readlines()\n",
        "\n",
        "    print(\"Sampling: \" + conn.name)\n",
        "    for line in lines[0:5]:  \n",
        "      print(line, end=\"\")\n",
        "\n",
        "    print(\"Lines: \" + str(len(lines)))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCwbvjSjWiuk"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/GIT/pretraining-bea2019/training/bpe/train.bpe.src\", \"r+\") as f:\n",
        "  print(len(f.readlines()))\n",
        "with open(\"/content/GIT/pretraining-bea2019/training/bpe/train.bpe.trg\", \"r+\") as f:\n",
        "  print(len(f.readlines()))\n",
        "with open(\"/content/GIT/pretraining-bea2019/training/bpe/valid.bpe.src\", \"r+\") as f:\n",
        "  print(len(f.readlines()))\n",
        "with open(\"/content/GIT/pretraining-bea2019/training/bpe/valid.bpe.trg\", \"r+\") as f:\n",
        "  print(len(f.readlines()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building Vocabularies with SentencePiece"
      ],
      "metadata": {
        "id": "JEHJhX91OnKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE_SRC = 14506\n",
        "VOCAB_SIZE_TRG = 13882\n",
        "\n",
        "if domain_error_adaptation:\n",
        "  if large_vocabs:\n",
        "    VOCAB_SIZE_SRC = 30776\n",
        "    VOCAB_SIZE_TRG = 22444\n",
        "  elif tied_embeddings:\n",
        "    VOCAB_SIZE_SRC = 22000\n",
        "    VOCAB_SIZE_TRG = 22000\n",
        "\n",
        "print(\"Vocabulary sizes set to: \\nSOURCE: {0}\\nTARGET: {1}\".format(VOCAB_SIZE_SRC,VOCAB_SIZE_TRG))"
      ],
      "metadata": {
        "id": "j2kOJSJTjnJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZLfqxMLQJFM"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/GIT/pretraining-bea2019/training/vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtLqle-YOqiF"
      },
      "outputs": [],
      "source": [
        "MARIAN_VOCAB = \"/content/GIT/pretraining-bea2019/systems/tools/marian-dev/build/marian-vocab\"\n",
        "VOCAB_DIR = \"/content/GIT/pretraining-bea2019/training/vocab\"\n",
        "\n",
        "TRAIN_SRC = \"/content/GIT/pretraining-bea2019/training/train.src\"\n",
        "TRAIN_TRG = \"/content/GIT/pretraining-bea2019/training/train.trg\"\n",
        "VALID_SRC = \"/content/GIT/pretraining-bea2019/training/valid.src\"\n",
        "VALID_TRG = \"/content/GIT/pretraining-bea2019/training/valid.trg\"\n",
        "\n",
        "TRAIN_BPE_SRC = \"/content/GIT/pretraining-bea2019/training/bpe/train.bpe.src\"\n",
        "TRAIN_BPE_TRG = \"/content/GIT/pretraining-bea2019/training/bpe/train.bpe.trg\"\n",
        "VALID_BPE_SRC = \"/content/GIT/pretraining-bea2019/training/bpe/valid.bpe.src\"\n",
        "VALID_BPE_TRG = \"/content/GIT/pretraining-bea2019/training/bpe/valid.bpe.trg\"\n",
        "\n",
        "\n",
        "!cat $TRAIN_SRC $VALID_SRC | $MARIAN_VOCAB > $VOCAB_DIR/vocab.src.yml\n",
        "!cat $TRAIN_TRG $VALID_TRG | $MARIAN_VOCAB > $VOCAB_DIR/vocab.trg.yml\n",
        "!cat $TRAIN_SRC $VALID_SRC $TRAIN_TRG $VALID_TRG | $MARIAN_VOCAB > $VOCAB_DIR/vocab.src_trg.yml\n",
        "\n",
        "!cat $TRAIN_BPE_SRC $VALID_BPE_SRC | $MARIAN_VOCAB > $VOCAB_DIR/vocab.bpe.src.yml\n",
        "!cat $TRAIN_BPE_TRG $VALID_BPE_TRG | $MARIAN_VOCAB > $VOCAB_DIR/vocab.bpe.trg.yml\n",
        "!cat $TRAIN_BPE_SRC $VALID_BPE_SRC $TRAIN_BPE_TRG $VALID_BPE_TRG | $MARIAN_VOCAB > $VOCAB_DIR/vocab.bpe.src_trg.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a slight hack here because Marian needs to recognise a particular file extension so that it could add its own custom required tokens to the vocabularies. These tokens are mandatory otherwise the pipeline wouldn't work."
      ],
      "metadata": {
        "id": "ycH8kR-ZOuTF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNOkUpDkyC6N"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/GIT/pretraining-bea2019/training/vocab/hack_needed/*\n",
        "!rm -rf /content/GIT/pretraining-bea2019/training/vocab/spm*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-uhzHml8kTS"
      },
      "outputs": [],
      "source": [
        "%cd /content/GIT/pretraining-bea2019/training/vocab\n",
        "\n",
        "!mkdir -p hack_needed\n",
        "%cd hack_needed\n",
        "\n",
        "!rm -rf /content/GIT/pretraining-bea2019/training/vocab/hack_needed/*\n",
        "!rm -rf /content/GIT/pretraining-bea2019/training/vocab/spm*\n",
        "\n",
        "SPM_DECODE = \"/content/GIT/pretraining-bea2019/systems/tools/marian-dev/build/spm_decode\"\n",
        "SPM_ENCODE = \"/content/GIT/pretraining-bea2019/systems/tools/marian-dev/build/spm_encode\"\n",
        "SPM_EXPORT_VOCAB = \"/content/GIT/pretraining-bea2019/systems/tools/marian-dev/build/spm_export_vocab\"\n",
        "SPM_NORMALIZE = \"/content/GIT/pretraining-bea2019/systems/tools/marian-dev/build/spm_normalize\"\n",
        "SPM_TRAIN = \"/content/GIT/pretraining-bea2019/systems/tools/marian-dev/build/spm_train\"\n",
        "\n",
        "!sed 's/\\r$//' /content/GIT/pretraining-bea2019/training/src_tokenised.txt > \\\n",
        "   /content/GIT/pretraining-bea2019/training/src_tokenised_prep.txt\n",
        "!sed 's/\\r$//' /content/GIT/pretraining-bea2019/training/trg_tokenised.txt > \\\n",
        "   /content/GIT/pretraining-bea2019/training/trg_tokenised_prep.txt\n",
        "\n",
        "!$SPM_TRAIN --input /content/GIT/pretraining-bea2019/training/src_tokenised_prep.txt --vocab_size $VOCAB_SIZE_SRC  --model_prefix spm.src --model_type=word\n",
        "!$SPM_TRAIN --input /content/GIT/pretraining-bea2019/training/trg_tokenised_prep.txt --vocab_size $VOCAB_SIZE_TRG  --model_prefix spm.trg --model_type=word\n",
        "\n",
        "!cp spm.src.vocab ../spm.src.yml\n",
        "!cp spm.src.model ../spm.src.spm\n",
        "!cp spm.trg.vocab ../spm.trg.yml\n",
        "!cp spm.trg.model ../spm.trg.spm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVYH7x05gufa"
      },
      "source": [
        "#### Source Word Corruption\n",
        "\n",
        "This part of the Notebook genereates corrupted source file if source word corruption is active."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zattzhm0gsMZ"
      },
      "outputs": [],
      "source": [
        "if source_word_corruption:\n",
        "\n",
        "  %cd /content/GIT/ICS5200/Tools/\n",
        "\n",
        "  from synthesis import Synthesizer, SynthesisStrategy\n",
        "\n",
        "  with open(\"/content/GIT/pretraining-bea2019/training/train.src\", \"r\") as f:\n",
        "    src_train_tokens = list(map(lambda x: x.replace('\\n', '').split(\" \"), f.readlines()))\n",
        "    \n",
        "    print(\"\\nSAMPLE FROM SOURCE\\n\")\n",
        "    for x in src_train_tokens[0:10]:\n",
        "      print(\" \".join(x))\n",
        "\n",
        "  print(\"\\n\")\n",
        "  synth = Synthesizer(src_train_tokens)\n",
        "  synth.synthesize(SynthesisStrategy.ORGANISED_DROPOUT, sentence_seed=1, token_seed=1, dropout_modulus=10)\n",
        "\n",
        "  print(\"\\nSAMPLE FROM SYNTHESISER\\n\")\n",
        "  for x in synth.data[0:10]:\n",
        "    print(\" \".join(x))\n",
        "\n",
        "  with open(\"/content/GIT/pretraining-bea2019/training/train.corrupt.src\", \"w\") as f:\n",
        "    \n",
        "    for swd_tokens in synth.data:\n",
        "      f.writelines(\" \".join(swd_tokens) + \"\\n\")\n",
        "\n",
        "  print(\"\\nWrote to file...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTDM5kfZxXQt"
      },
      "source": [
        "# HuggingFace BERT\n",
        "\n",
        "This part of the Notebook sources the pretrained BERT models. Note that the size of the embedding vectors need to be reworked to fit with the smaller vocabulary sizes of the Maltese GEC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQqxsRBUxgvh"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l6Bjl52X1Gb"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/GIT/pretraining-bea2019/systems/model.bert/*\n",
        "!mkdir -p /content/GIT/pretraining-bea2019/systems/model.bert\n",
        "%cd /content/GIT/ICS5200/Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3p1vb-myIVg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "# auth_token = \"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\"\n",
        "\n",
        "from HF2M import ModelConverter\n",
        "\n",
        "for bert in ['BERTu','mBERTu']:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"MLRS/{0}\".format(bert))\n",
        "  model = AutoModelForMaskedLM.from_pretrained(\"MLRS/{0}\".format(bert))\n",
        "  model.resize_token_embeddings(VOCAB_SIZE_SRC)\n",
        "\n",
        "  !rm -rf /content/GIT/pretraining-bea2019/systems/model.lowresource.mt/$bert.npz\n",
        "  %cd /content/GIT/ICS5200/Tools/\n",
        "\n",
        "  MC = ModelConverter()\n",
        "  MC.feed_model(model)\n",
        "  MC.translate_model(\"/content/GIT/pretraining-bea2019/systems/model.bert/{0}.npz\".format(bert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpTqoTpaXyBw"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you need to, you can run the command hereunder to clear any existing model files in your folders."
      ],
      "metadata": {
        "id": "nql_8Pq4Q2iw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n02aVtkhnVEn"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/GIT/pretraining-bea2019/systems/model.lowresource.mt/*\n",
        "%cd /content/GIT/pretraining-bea2019/systems/model.lowresource.mt/\n",
        "!mkdir -p log"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **KEY** - Determines which architecture the model shall be based on. You have three options:\n",
        "  * **AMUN** - Nematus equivalent transformer architecture\n",
        "  * **S2S** - Sequence-to-Sequence architecture\n",
        "  * **Transformer** - Vaswani-style transformer architecture\n",
        "* **NETWORK_NAME_SUFFIX** - This is just a friendly suffix so that you could distinguish different models in your Google Drive. The folder names will always be a concatenation of the KEY and the NETWORK_NAME_SUFFIX\n",
        "* **EPOCH_INTERVAL** - This number determines after how many epochs the model will save a checkpoint to the Google Drive. Applicable only if **ITERATE_ON_EPOCH** is set to True.\n",
        "* **UPDATE_INTERVAL** - This number determines after how many batch updates the model will save a checkpoint to the Google Drive. Applicable only if **ITERATE_ON_EPOCH** is set to False.\n",
        "* **ITERATE_ON_EPOCH** - Toggle between iterating on epochs or batch updates.\n",
        "\n",
        "*Note: We recommend that you do not save checkpoints based on epochs! Epochs can become very long when dealing with EncDec architectures and it could take days to elapse a single epoch, especially when working with the larger models such as the Pretrained ones. You can leave the settings for **ITERATE_ON_EPOCH**, **EPOCH_INTERVAL** and **UPDATE_INTERVAL** exactly as is (False, 1, 500).*"
      ],
      "metadata": {
        "id": "rf838zMuRFC2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44DxiUcWUWL0"
      },
      "outputs": [],
      "source": [
        "KEY = \"AMUN\" #@param {type:\"string\"}\n",
        "\n",
        "NETWORK_NAME_SUFFIX = \"_TEST\" #@param {type:\"string\"}\n",
        "NETWORK_NAME = KEY + NETWORK_NAME_SUFFIX\n",
        "\n",
        "EPOCH_INTERVAL = 1 #@param {type:\"integer\"}\n",
        "UPDATE_INTERVAL = 500 #@param {type:\"integer\"}\n",
        "\n",
        "ITERATE_ON_EPOCH = False #@param {type:\"boolean\"}\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/MARIAN-CHECKPOINTS\n",
        "!mkdir -p /content/drive/MyDrive/MARIAN-CHECKPOINTS/$NETWORK_NAME\n",
        "\n",
        "%cd /content/GIT/pretraining-bea2019/systems\n",
        "\n",
        "units = \"epochs\" if ITERATE_ON_EPOCH else \"updates\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are starting training, then you can set **ITER** to 0. If you are continuing from a different day, then check the last checkpoint that was generated in you Google Drive and input that number into **ITER**. Marian will then copy the checkpoint from Google Drive back into the current session and resume training from that checkpoint.\n",
        "\n",
        "**ITER_MAX** determines the point when training will stop.\n",
        "\n",
        "*Note: Both **ITER** and **ITER_MAX** need to be numbers that make sense. If working with epochs, these would be smaller integers. But since we are working with batch iterations, they are larger integers.*"
      ],
      "metadata": {
        "id": "L2BRf9hYTJbA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0tYPmk4j75I"
      },
      "outputs": [],
      "source": [
        "#@title Set Iteration Parameters\n",
        "ITER =  0#@param\n",
        "ITER_MAX = 500 #@param\n",
        "\n",
        "print(\"NETWORK: {0}\".format(NETWORK_NAME))\n",
        "print(\"TYPE: {0}\".format(TYPE))\n",
        "print(\"START: {0} {1}\".format(ITER, units))\n",
        "print(\"END: {0} {1}\".format(ITER_MAX, units))\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ARCH_PARAM = 's2s' if pretrained else KEY.lower()\n",
        "TRAIN_FROM_PARAM = \"train.corrupt.src\" if source_word_corruption else \"train.src\"\n",
        "\n",
        "# For any model to work, it must at least have at least have the target embeddings tied (--tied-embeddings)\n",
        "# The adaptation (tied-embeddings-all) ties both the source and the target embeddings.\n",
        "TIED_EMBEDDINGS_PARAM = \"tied-embeddings-all\" if tied_embeddings else \"tied-embeddings\"\n",
        "\n",
        "PRETRAINED_PARAM = \"\"\n",
        "\n",
        "if pretrained == 1:\n",
        "  PRETRAINED_PARAM = \"--pretrained-model model.bert/BERTu.npz\"\n",
        "elif pretrained == 2:\n",
        "  PRETRAINED_PARAM = \"--pretrained-model model.bert/mBERTu.npz\"\n",
        "\n",
        "DIM_EMB_PARAM = 768 if pretrained else 512\n",
        "\n",
        "print(\"ARCHITECTURE_PARAM: {0}{1}\".format(ARCH_PARAM, \" (..must use S2S when pretraining)\" if pretrained else \"\"))\n",
        "print(\"TRAIN_FROM_PARAM: {0}\".format(TRAIN_FROM_PARAM))\n",
        "print(\"TIED_EMBEDDINGS_PARAM: {0}\".format(TIED_EMBEDDINGS_PARAM))\n",
        "print(\"DIM_EMB_PARAM: {0}\".format(DIM_EMB_PARAM))\n",
        "print(\"PRETRAINED_PARAM: {0}\".format(PRETRAINED_PARAM))\n",
        "print(\"\\nVocabulary sizes set to: \\nSOURCE: {0}\\nTARGET: {1}\".format(VOCAB_SIZE_SRC,VOCAB_SIZE_TRG))"
      ],
      "metadata": {
        "id": "D2PJDkf_b63V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HoZ7JFeGR1i"
      },
      "outputs": [],
      "source": [
        "# COPY FROM DRIVE\n",
        "if ITER != 0:\n",
        "  SRC = str(ITER).zfill(2) if ITERATE_ON_EPOCH else str(ITER).zfill(5)\n",
        "  !cp -r /content/drive/MyDrive/MARIAN-CHECKPOINTS/$NETWORK_NAME/$SRC/* /content/GIT/pretraining-bea2019/systems/model.lowresource.mt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please pay close attention to platform-specific configurations like **cpu-threads**!\n",
        "\n",
        "Also, don't forget to monitor **ITER**. If you commenced training, that value would update."
      ],
      "metadata": {
        "id": "R1MmBZTtbs6U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DLN_6x1ttPO"
      },
      "outputs": [],
      "source": [
        "%cd /content/GIT/pretraining-bea2019/systems\n",
        "from IPython.display import clear_output\n",
        "\n",
        "print(\"ITER: {0}\".format(ITER))\n",
        "print(\"ITER_MAX: {0}\".format(ITER_MAX))\n",
        "\n",
        "while(ITER < ITER_MAX):\n",
        "\n",
        "  if ITERATE_ON_EPOCH: \n",
        "    LAST_CHKP = str(ITER).zfill(2)\n",
        "    ITER += EPOCH_INTERVAL\n",
        "    NEXT_CHKP = str(ITER).zfill(2)\n",
        "    AFTER = \"{0}e\".format(ITER)\n",
        "  else:\n",
        "    LAST_CHKP = str(ITER).zfill(5)\n",
        "    ITER += UPDATE_INTERVAL\n",
        "    NEXT_CHKP = str(ITER).zfill(5)\n",
        "    AFTER = \"{0}u\".format(ITER)\n",
        "\n",
        "    # --ignore-model-config \\  --no-restore-corpus \\    --dim-vocabs 22000 \\\n",
        "    # --pretrained-model model.bert/mBERTu.npz \\\n",
        "\n",
        "  !tools/marian-dev/build/marian --model model.lowresource.mt/model.npz \\\n",
        "    --type $ARCH_PARAM \\\n",
        "    --cpu-threads 8 \\\n",
        "    $PRETRAINED_PARAM \\\n",
        "    --train-sets ../training/$TRAIN_FROM_PARAM ../training/train.trg \\\n",
        "    --valid-sets ../training/valid.src ../training/valid.trg \\\n",
        "    --vocabs ../training/vocab/spm.src.spm ../training/vocab/spm.trg.spm \\\n",
        "    --dim-emb $DIM_EMB_PARAM \\\n",
        "    --mini-batch 4 \\\n",
        "    --maxi-batch 100 \\\n",
        "    --max-length 100 \\\n",
        "    -w 10000 \\\n",
        "    --layer-normalization \\\n",
        "    --dropout-rnn 0.2 \\\n",
        "    --dropout-src 0.1 \\\n",
        "    --dropout-trg 0.1 \\\n",
        "    --$TIED_EMBEDDINGS_PARAM \\\n",
        "    --no-restore-corpus \\\n",
        "    --after $AFTER \\\n",
        "    --seed 1111 \\\n",
        "    --overwrite --keep-best --exponential-smoothing \\\n",
        "    --normalize=1 --beam-size 6 \\\n",
        "    --valid-freq 500 \\\n",
        "    --save-freq 500u \\\n",
        "    --disp-freq 500 \\\n",
        "    --disp-label-counts \\\n",
        "    --learn-rate 0.0001 \\\n",
        "    --lr-report \\\n",
        "    --optimizer-params 0.9 0.98 1e-08 \\\n",
        "    --clip-norm 5 \\\n",
        "    --early-stopping 5 \\\n",
        "    --valid-metrics ce-mean-words cross-entropy perplexity bleu \\\n",
        "    --cost-type=ce-mean-words \\\n",
        "    --valid-reset-stalled \\\n",
        "    --log model.lowresource.mt/log/train.log \\\n",
        "    --valid-log model.lowresource.mt/log/valid.log \\\n",
        "    --log-level trace\n",
        "\n",
        "  # clear_output()\n",
        "  !mkdir -p /content/drive/MyDrive/MARIAN-CHECKPOINTS/$NETWORK_NAME/$NEXT_CHKP\n",
        "  !cp -r /content/GIT/pretraining-bea2019/systems/model.lowresource.mt/* /content/drive/MyDrive/MARIAN-CHECKPOINTS/$NETWORK_NAME/$NEXT_CHKP\n",
        "\n",
        "  print(\"LAST: {0} - THIS: {1} - PARAM: {2}\".format(LAST_CHKP, NEXT_CHKP, AFTER))\n",
        "  print(\"SAVE FOLDER: {0}/{1}\".format(NETWORK_NAME, NEXT_CHKP))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trained Models\n",
        "\n",
        "These cells copy over the trained models from our study."
      ],
      "metadata": {
        "id": "EL0WFamy8RUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!mkdir -p TrainedModels"
      ],
      "metadata": {
        "id": "_JyUPM9J8vRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd TrainedModels"
      ],
      "metadata": {
        "id": "9eLeo6tz85-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1E009yHoVW8F1WJc_u1gemuSc8MkvP9Qy\n",
        "!gdown https://drive.google.com/uc?id=1rtdAqRdRzlMrM2fcVP4dyQdjshSszm10\n",
        "!gdown https://drive.google.com/uc?id=1HP0gaDcHmh52tQk0PjTznYuJuXrueiiK\n",
        "!gdown https://drive.google.com/uc?id=1o7GROxHfjirzZasvqJjIWL4_KpZJArn3\n",
        "!gdown https://drive.google.com/uc?id=1iM47ATQOL0ffBWQILc3TyhqIHTcZul5e\n",
        "!gdown https://drive.google.com/uc?id=12HfCmJXFisbn6nOqtW6kx-YjKoFnPWz2\n",
        "!gdown https://drive.google.com/uc?id=1K7zrm4PbaqWX2QG7b7ATHrANB-ZmIA96\n",
        "!gdown https://drive.google.com/uc?id=1EO1c3jl4qbfBGVHUB37rgnSBZPGXjoJD\n",
        "!gdown https://drive.google.com/uc?id=1U_jdvJoEE8SIIjAxIRyGSSMts_W-LR2g\n",
        "!gdown https://drive.google.com/uc?id=1UAsJOjkjjBvwrJWibp2Q--fjJzoefbDS\n",
        "!gdown https://drive.google.com/uc?id=1cisBb5XhrJmjeFep6Zs9f3Qk3T2JZPYv\n",
        "!gdown https://drive.google.com/uc?id=1sggRQV17q2n2QrKjlOu2_Ah-5IWpJjum\n",
        "!gdown https://drive.google.com/uc?id=1Q_4s-9Sq2q0j_YnTt4oFVhAq5FOuWc0c"
      ],
      "metadata": {
        "id": "oabKQp2n8X3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO7FIkeWkN4D"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "This part of the Notebook is dedicated to the Evaluation phases of the study."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2GXZHkukf5-"
      },
      "source": [
        "#### Recall Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDcYZ13Ls3kd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYJpH5Qgrdwc"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/MARIAN-CHECKPOINTS/EVAL-MODELS\n",
        "\n",
        "%cd /content/\n",
        "!mkdir -p Evaluation\n",
        "!mkdir -p Evaluation/Models\n",
        "!mkdir -p Evaluation/Decoder\n",
        "!mkdir -p Evaluation/Vocabs\n",
        "!mkdir -p Evaluation/Source\n",
        "!mkdir -p Evaluation/Target\n",
        "!mkdir -p /content/drive/MyDrive/MARIAN-DECODE\n",
        "!mkdir -p /content/drive/MyDrive/MARIAN-VOCABS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training, we placed all the best performing models in a folder called EVAL-MODELS. We have provided these models via the links in the previous section and the are instead located in the *TrainedModels* folder."
      ],
      "metadata": {
        "id": "xHzpX27w-j-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -a \"/content/TrainedModels\" \"/content/Evaluation/Models/\""
      ],
      "metadata": {
        "id": "5BDbnXSd-WS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp8bpwvFrpjt"
      },
      "outputs": [],
      "source": [
        "# You can skip this if you don't have any.\n",
        "!cp -a \"/content/drive/MyDrive/MARIAN-CHECKPOINTS/EVAL-MODELS/.\" \"/content/Evaluation/Models/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oAmLFsR7KH3"
      },
      "outputs": [],
      "source": [
        "!cp -a \"/content/GIT/ICS5200/Evaluation Vocabs/.\" \"/content/Evaluation/Vocabs/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The list hereunder will determine which models will be evaluated. For the sake of demonstration, only *Final-Model* has been included, but all of the other models can be passed here. They can be added by appending elements to the **decoding_models** list. For example, this is how we can add \"Tied-Embeddings.npz\": \n",
        "\n",
        "['Final-Model','Tied-Embeddings']"
      ],
      "metadata": {
        "id": "L7yw7Fov_AVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoding_models = ['Final-Model']"
      ],
      "metadata": {
        "id": "l-cvm_Za--Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdSptZz7zjn5"
      },
      "source": [
        "#### Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKj4vrZUrR2Y"
      },
      "outputs": [],
      "source": [
        "%cd /content/GIT/ICS5200/Tools/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om4qlzs_4-NG"
      },
      "outputs": [],
      "source": [
        "MARIAN_DECODE_DIR = \"/content/drive/MyDrive/MARIAN-DECODE\"\n",
        "MARIAN_VOCAB_DIR = \"/content/drive/MyDrive/MARIAN-VOCABS\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66nHToWW4Iyb"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_splitter\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We technically already loaded the tokeniser, but we included this for convenience in the event that validation is being performed on its own."
      ],
      "metadata": {
        "id": "dwffcSA1WVFo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1-7TMA2rR2Z"
      },
      "outputs": [],
      "source": [
        "from tokenisation import MTRegex, MTParTokenizer, MTWordTokenizer, MTSentenceTokenizer, MTSentencePieceTokenizer\n",
        "import urllib.parse\n",
        "import requests\n",
        "from IPython.display import clear_output\n",
        "\n",
        "tokeniser = MTWordTokenizer();\n",
        "tokeniser_api_link = \"https://mlrs.research.um.edu.mt/tools/mlrsapi/tokenise?text=\"\n",
        "\n",
        "def tokenise(path, use_rest=False):\n",
        "\n",
        "  tokens = []\n",
        "  with open(path, 'r', errors='replace') as f:\n",
        "\n",
        "    lines = f.readlines()\n",
        "    count = 0\n",
        "\n",
        "    for line in lines:\n",
        "\n",
        "      if (use_rest):\n",
        "        query = \"https://mlrs.research.um.edu.mt/tools/mlrsapi/tokenise?text=\" + urllib.parse.quote(line)\n",
        "        response = requests.get(query)\n",
        "        tokens.append(response.json()['result'])\n",
        "      else:\n",
        "        tokens.append(tokeniser.tokenize(line))\n",
        "      \n",
        "      count += 1\n",
        "      print(\"{0}/{1}\".format(count, len(lines)))\n",
        "\n",
        "    clear_output()\n",
        "    print(\"Processed: \" + path)\n",
        "  \n",
        "  return tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some general steps related to setup."
      ],
      "metadata": {
        "id": "VSSVWyffWl3A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7nUaNRYq5CZ"
      },
      "outputs": [],
      "source": [
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em_p-sNDtzYV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "model_directories = []\n",
        "\n",
        "!rm -rf Evaluation/Decoder/*\n",
        "\n",
        "for file in os.listdir(\"/content/Evaluation/Models/\"):\n",
        "  DIR_NAME = file.replace('.npz', '')\n",
        "\n",
        "  model_directories.append(DIR_NAME)\n",
        "  !mkdir -p Evaluation/Decoder/$DIR_NAME\n",
        "  !mkdir -p Evaluation/Decoder/$DIR_NAME/Log\n",
        "  !mkdir -p Evaluation/Decoder/$DIR_NAME/Output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glob.glob('/content/GIT/ICS5200/QariTalProvi_Batch_2/Training/{0}/Target/*.txt'.format('Converted' if full_set else 'Subset'))"
      ],
      "metadata": {
        "id": "jFB5uXITFlAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGGFs7BTBkOA"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "EVALS_DIR = '/content/Evaluation/Decoder/'\n",
        "MODELS_DIR = '/content/Evaluation/Models/'\n",
        "SOURCE_DIR = '/content/Evaluation/Source/'\n",
        "TARGET_DIR = '/content/Evaluation/Target/'\n",
        "REFERENCES_DIR = '/content/Evaluation/References/'\n",
        "M2_DIR = \"/content/Evaluation/M2/\"\n",
        "SCORES_DIR = \"/content/Evaluation/Scores/\"\n",
        "DECODER_DIR = '/content/GIT/pretraining-bea2019/systems/tools/marian-dev/build/marian-decoder'\n",
        "SCORER_DIR = '/content/GIT/pretraining-bea2019/systems/tools/marian-dev/build/marian-scorer'\n",
        "\n",
        "full_set = False\n",
        "source_listdir = glob.glob('/content/GIT/ICS5200/QariTalProvi_Batch_2/Training/{0}/Source/*.txt'.format('Converted' if full_set else 'Subset'))\n",
        "target_listdir = glob.glob('/content/GIT/ICS5200/QariTalProvi_Batch_2/Training/{0}/Target/*.txt'.format('Converted' if full_set else 'Subset'))\n",
        "\n",
        "source_listdir.sort()\n",
        "target_listdir.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NBnhSNNBlzQ"
      },
      "outputs": [],
      "source": [
        "!rm -rf $SOURCE_DIR/*\n",
        "!rm -rf $TARGET_DIR/*\n",
        "\n",
        "for source_file in source_listdir:\n",
        "  token_list = tokenise(source_file)\n",
        "\n",
        "  new_file = SOURCE_DIR + source_file.split(\"/\")[-1:][0].replace('txt','tokenised.in')\n",
        "  \n",
        "  with open(new_file, \"w\", encoding=\"utf-8\") as f:\n",
        "      print(\"Writing to file: {0}\".format(new_file))\n",
        "\n",
        "      f.write(\"\\n\".join(list(map(lambda x: \" \".join(x), token_list))))\n",
        "\n",
        "for target_file in target_listdir:\n",
        "  token_list = tokenise(target_file)\n",
        "\n",
        "  new_file = TARGET_DIR + target_file.split(\"/\")[-1:][0].replace('txt','tokenised.ref')\n",
        "  \n",
        "  with open(new_file, \"w\", encoding=\"utf-8\") as f:\n",
        "      print(\"Writing to file: {0}\".format(new_file))\n",
        "\n",
        "      f.write(\"\\n\".join(list(map(lambda x: \" \".join(x), token_list))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onm7K6bDFQb_"
      },
      "outputs": [],
      "source": [
        "tokenised_source_listdir = glob.glob(SOURCE_DIR + '/*.tokenised.in')\n",
        "tokenised_source_listdir.sort()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUMia-jAr7Fw"
      },
      "source": [
        "#### Translation\n",
        "\n",
        "Requires Marian build."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI4_xH-_kPgn"
      },
      "outputs": [],
      "source": [
        "# GENERATE\n",
        "%cd /content\n",
        "\n",
        "from os.path import exists\n",
        "\n",
        "generate_from_scratch = False\n",
        "\n",
        "for model_directory in decoding_models:\n",
        "  \n",
        "  CURRENT_MODEL_DIR = MODELS_DIR + model_directory + \".npz\"\n",
        "  OUTPUT_DIR = EVALS_DIR + model_directory + \"/Output/\"\n",
        "  LOG_DIR = EVALS_DIR + model_directory + \"/Log/\"\n",
        "  SRC_VOCAB = \"/content/Evaluation/Vocabs/bench.src.spm\" if model_directory in  ['Benchmark-AMUN','Benchmark-S2S','Benchmark-Transformer', 'SRC-Corruption','Benchmark-Tied-Embeddings','Benchmark-Pretrained-BERTu','Benchmark-Pretrained-mBERTu'] \\\n",
        "    else \"/content/Evaluation/Vocabs/22K.src.spm\"\n",
        "  TRG_VOCAB = \"/content/Evaluation/Vocabs/bench.trg.spm\" if model_directory in  ['Benchmark-AMUN','Benchmark-S2S','Benchmark-Transformer', 'SRC-Corruption','Benchmark-Tied-Embeddings','Benchmark-Pretrained-BERTu','Benchmark-Pretrained-mBERTu'] \\\n",
        "    else \"/content/Evaluation/Vocabs/22K.trg.spm\"\n",
        "\n",
        "  print(\"*** {0} ***\".format(model_directory))\n",
        "  print(\"Model File: {0}\".format(CURRENT_MODEL_DIR))\n",
        "  print(\"Output Dir: {0}\".format(OUTPUT_DIR))\n",
        "  print(\"  Log File: {0}\\n\".format(LOG_DIR))\n",
        "\n",
        "  c = 0\n",
        "\n",
        "  if generate_from_scratch:\n",
        "\n",
        "    for s, t in zip(tokenised_source_listdir, target_listdir):\n",
        "      c += 1\n",
        "\n",
        "      print(\"Processing pair {2}:\\n\\tSORUCE: {0}\\n\\tTARGET: {1}\".format(s, t, c))\n",
        "      LOG_FILE = LOG_DIR + \"PAIR-{0:03}.log\".format(c)\n",
        "      OUTPUT_FILE = OUTPUT_DIR + \"PAIR-{0:03}.out\".format(c)\n",
        "\n",
        "      !$DECODER_DIR -m $CURRENT_MODEL_DIR -n --cpu-threads 5  -i '$s' -o $OUTPUT_FILE --log $LOG_FILE --vocabs $SRC_VOCAB $TRG_VOCAB \n",
        "      print(\"\\tLogged to {0}\".format(LOG_FILE))\n",
        "    \n",
        "    print(\"\\tPersisting to Google Drive... {0}/{1}\".format(MARIAN_DECODE_DIR, model_directory))\n",
        "\n",
        "    !mkdir -p $MARIAN_DECODE_DIR/$model_directory\n",
        "    !cp $OUTPUT_DIR/* $MARIAN_DECODE_DIR/$model_directory\n",
        "\n",
        "  else:\n",
        "\n",
        "    print(\"Recovering from Google Drive...\")\n",
        "    !cp  $MARIAN_DECODE_DIR/$model_directory/* $OUTPUT_DIR/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsC-_f8yFvaB"
      },
      "source": [
        "### ERRANT Scoring"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation"
      ],
      "metadata": {
        "id": "ihRhYJCXIKZ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTXqlH-UiwRb"
      },
      "outputs": [],
      "source": [
        "%cd /content/GIT/ICS5200/Tools\n",
        "\n",
        "from M2_reference_generation import generate_m2_reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KI6-XAXF5_R"
      },
      "outputs": [],
      "source": [
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIfFqt69Hauy"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GbTe3Y6ImV3"
      },
      "outputs": [],
      "source": [
        "!pip install errant"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Genereate Reference Files\n",
        "\n",
        "We generate MaxMatch (M2) reference files from our target folder."
      ],
      "metadata": {
        "id": "NYP5p0bzIQan"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvRUZMTEjBiF"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "REFERENCES_DIR = \"/content/Evaluation/References/\"\n",
        "!rm -rf $REFERENCES_DIR\n",
        "!mkdir -p $REFERENCES_DIR\n",
        "\n",
        "for filepath in glob.glob(TARGET_DIR + \"*\"):\n",
        "  filename = filepath.split(\"/\")[-1:][0].replace(\"ref\", \"errant.ref\")\n",
        "  print(\"Generating: {0}\".format(filename))\n",
        "  generate_m2_reference(filepath, REFERENCES_DIR + filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxdCMhgzU7US"
      },
      "outputs": [],
      "source": [
        "target_listdir = glob.glob(TARGET_DIR + \"*.ref\")\n",
        "target_listdir.sort()\n",
        "\n",
        "c = 0\n",
        "for x in target_listdir:\n",
        "  c += 1\n",
        "  print('{0}: {1}'.format(c,x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhFY9m1aIJIV"
      },
      "outputs": [],
      "source": [
        "# PAD empty translations\n",
        "\n",
        "for model_directory in decoding_models:\n",
        "  OUTPUT_DIR = EVALS_DIR + model_directory + \"/Output/\"\n",
        "  \n",
        "  output_listdir = glob.glob(OUTPUT_DIR + \"*.out\")\n",
        "  output_listdir.sort()\n",
        "\n",
        "  for o in output_listdir:\n",
        "    with open(o, \"r\", encoding=\"utf-8\") as f:\n",
        "      lines = f.read()\n",
        "\n",
        "    revised_lines = list(map(lambda x: x if x else '<NA>', lines.split('\\n')))\n",
        "    with open(o, \"w\", encoding=\"utf-8\") as f:\n",
        "      f.write('\\n'.join(revised_lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare Translations\n",
        "\n",
        "We compare our model-generated translations against our reference files."
      ],
      "metadata": {
        "id": "uavVQ02XIh5H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAErRrxNqxd0"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "for model_directory in decoding_models:\n",
        "\n",
        "  OUTPUT_DIR = EVALS_DIR + model_directory + \"/Output/\"\n",
        "  \n",
        "  output_listdir = glob.glob(OUTPUT_DIR + \"*.out\")\n",
        "  reference_listdir = glob.glob(TARGET_DIR + \"*.tokenised.ref\")\n",
        "\n",
        "  output_listdir.sort()\n",
        "  reference_listdir.sort()\n",
        "\n",
        "  !rm -rf $M2_DIR$model_directory\n",
        "  !mkdir -p $M2_DIR$model_directory\n",
        "  for o in output_listdir:\n",
        "\n",
        "    filename = o.split(\"/\")[-1:][0].replace(\"out\", \"tagged.m2\")\n",
        "    iter = filename.split(\".\")[0][-3:]\n",
        "\n",
        "    r = reference_listdir[int(iter)-1]\n",
        "\n",
        "    print(\"Comparing {0} to {1}...\".format(o, r))\n",
        "    !errant_parallel -orig \"$o\" -cor \"$r\" -out \"$M2_DIR$model_directory/$filename\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scoring\n",
        "\n",
        "Produce the scores for each comparison exercise."
      ],
      "metadata": {
        "id": "XyuiBjAnIEv4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI-esnCJ0qXt"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "!mkdir -p $SCORES_DIR\n",
        "\n",
        "for term in ['dt', 'ds', 'cs']:\n",
        "\n",
        "  print(\"\\nSCHEME: {0}\".format(term))\n",
        "\n",
        "  for model_directory in decoding_models:\n",
        "\n",
        "    SCORES_FILE = SCORES_DIR + model_directory + \".{0}.scores\".format(term)\n",
        "    !rm -rf $SCORES_FILE\n",
        "\n",
        "    tagged_listdir = glob.glob(M2_DIR + model_directory + \"/*.tagged.m2\")\n",
        "    references_listdir = glob.glob(REFERENCES_DIR + \"/*.errant.ref\" )\n",
        "\n",
        "    tagged_listdir.sort()\n",
        "    references_listdir.sort()\n",
        "\n",
        "    c = 0\n",
        "    for t in tagged_listdir:\n",
        "\n",
        "      iter = t.split(\"/\")[-1:][0].split(\".\")[0][-3:]\n",
        "      r = references_listdir[int(iter)-1]\n",
        "\n",
        "      c += 1\n",
        "      print(\"Scoring - {2:03} - {0} against {1}\".format(t, r, c))\n",
        "\n",
        "      !echo $t >> $SCORES_FILE\n",
        "      !errant_compare -hyp \"$t\" -ref \"$r\" -$term >> $SCORES_FILE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyJJeVYxmrae"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/MARIAN-SCORES"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Produce the total scores across all the files for the model. Set **persist** to True to save the output on Google Drive."
      ],
      "metadata": {
        "id": "Kka7gkd7RSUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persist = False #@param {type:\"boolean\"}"
      ],
      "metadata": {
        "id": "uqJ458WERXdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5d9CQV_LgqQ"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "schemes = {'dt': 'Token-Based Error Detection'\n",
        "  , 'ds': 'Span-Based Error Detection'\n",
        "  , 'cs': 'Span-Based Error Correction'}\n",
        "\n",
        "for term in ['dt', 'ds', 'cs']:\n",
        "\n",
        "  for model_directory in decoding_models:\n",
        "    score_file = SCORES_DIR + model_directory + '.{0}.scores'.format(term)\n",
        "\n",
        "    aggregates = []\n",
        "\n",
        "    with open(score_file, \"r\", encoding=\"utf-8\") as f:\n",
        "      lines = list(map(lambda x: x.replace('\\n', ''), f.readlines()))\n",
        "\n",
        "      counter = 0\n",
        "      current = {}\n",
        "      for i in range(0, len(lines)):\n",
        "\n",
        "        line = lines[i]\n",
        "\n",
        "        if counter == 0:\n",
        "          current['file'] = line\n",
        "        elif counter == 4:\n",
        "          arr_val = line.split('\\t')\n",
        "          if len(arr_val) < 6:\n",
        "            current['TP'] = 0\n",
        "            current['FP'] = 0\n",
        "            current['FN'] = 0\n",
        "            current['Prec'] = 0\n",
        "            current['Rec'] = 0\n",
        "            current['F0.5'] = 0\n",
        "          else:\n",
        "            current['TP'] = arr_val[0]\n",
        "            current['FP'] = arr_val[1]\n",
        "            current['FN'] = arr_val[2]\n",
        "            current['Prec'] = arr_val[3]\n",
        "            current['Rec'] = arr_val[4]\n",
        "            current['F0.5'] = arr_val[5]\n",
        "        elif counter == 6:\n",
        "          aggregates.append(current)\n",
        "          current = {}\n",
        "          counter = -1\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "    score_file_agg = score_file.replace(\"scores\", \"agg\")\n",
        "\n",
        "    with open(score_file_agg, \"w\", encoding=\"utf-8\") as f:\n",
        "      dict_writer = csv.DictWriter(f, aggregates[0].keys())\n",
        "      dict_writer.writeheader()\n",
        "      dict_writer.writerows(list(filter(lambda x: \"/content/Evaluation\" in x['file'], aggregates)))\n",
        "    \n",
        "    score_summary = score_file.replace(\"scores\", \"summary\")\n",
        "    \n",
        "    TP = sum(list(map(lambda x: int(x['TP']), aggregates)))\n",
        "    FP = sum(list(map(lambda x: int(x['FP']), aggregates)))\n",
        "    FN = sum(list(map(lambda x: int(x['FN']), aggregates)))\n",
        "    Prec = TP/(TP+FP)\n",
        "    Rec = TP/(TP+FN)\n",
        "    FScore = 1.25*((Prec*Rec)/(0.25*Prec+Rec))\n",
        "\n",
        "    prompts = [\n",
        "      \"{0}\".format(schemes[term]),\n",
        "      \"TP: {0}\".format(TP),\n",
        "      \"FP: {0}\".format(FP),\n",
        "      \"FN: {0}\".format(FN),\n",
        "      \"Precision: {0}\".format(Prec),\n",
        "      \"Recall: {0}\".format(Rec),\n",
        "      \"F0.5 Score: {0}\".format(FScore)]\n",
        "\n",
        "    with open(score_summary, \"w\", encoding=\"utf-8\") as f:\n",
        "      for prompt in prompts:\n",
        "        print(prompt)\n",
        "        f.write(prompt)\n",
        "\n",
        "    print()\n",
        "\n",
        "    if persist:\n",
        "      !mkdir -p /content/drive/MyDrive/MARIAN-SCORES/$model_directory/\n",
        "      !mkdir -p /content/drive/MyDrive/MARIAN-SCORES/$model_directory/SCORES/\n",
        "      !mkdir -p /content/drive/MyDrive/MARIAN-SCORES/$model_directory/AGGREGATES/\n",
        "      !mkdir -p /content/drive/MyDrive/MARIAN-SCORES/$model_directory/SUMMARY/\n",
        "      !cp '$score_file' '/content/drive/MyDrive/MARIAN-SCORES/$model_directory/SCORES/'\n",
        "      !cp '$score_file_agg' '/content/drive/MyDrive/MARIAN-SCORES/$model_directory/AGGREGATES/'\n",
        "      !cp '$score_summary' '/content/drive/MyDrive/MARIAN-SCORES/$model_directory/SUMMARY/'\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yFqvvbLk-C-U",
        "04YzyrbclvxF",
        "HSBjOWHbwkoZ",
        "LVYH7x05gufa",
        "CTDM5kfZxXQt",
        "ihRhYJCXIKZ-",
        "NYP5p0bzIQan"
      ],
      "name": "MGEC.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}